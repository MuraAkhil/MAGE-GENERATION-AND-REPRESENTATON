{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19687bc6-bb47-45e8-8f1a-8c0c65051b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Muraa\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Started Training and Visualization...\n",
      "Epoch [0/100] Batch 0/1875 Loss D: 0.6450, Loss G: 1.1256\n",
      "Epoch [0/100] Batch 625/1875 Loss D: 0.0001, Loss G: 9.4397\n",
      "Epoch [0/100] Batch 1250/1875 Loss D: 0.0000, Loss G: 10.7337\n",
      "Epoch [1/100] Batch 0/1875 Loss D: 0.0000, Loss G: 33.2033\n",
      "Epoch [1/100] Batch 625/1875 Loss D: 0.0000, Loss G: 15.1516\n",
      "Epoch [1/100] Batch 1250/1875 Loss D: 0.0000, Loss G: 46.0994\n",
      "Epoch [2/100] Batch 0/1875 Loss D: 0.0000, Loss G: 45.8353\n",
      "Epoch [2/100] Batch 625/1875 Loss D: 0.0000, Loss G: 45.4560\n",
      "Epoch [2/100] Batch 1250/1875 Loss D: 0.0000, Loss G: 45.3596\n",
      "Epoch [3/100] Batch 0/1875 Loss D: 0.0000, Loss G: 45.4050\n",
      "Epoch [3/100] Batch 625/1875 Loss D: 0.0000, Loss G: 45.4742\n",
      "Epoch [3/100] Batch 1250/1875 Loss D: 0.0000, Loss G: 45.2892\n",
      "Epoch [4/100] Batch 0/1875 Loss D: 0.0000, Loss G: 45.2095\n",
      "Epoch [4/100] Batch 625/1875 Loss D: 0.0000, Loss G: 44.9964\n",
      "Epoch [4/100] Batch 1250/1875 Loss D: 0.0000, Loss G: 44.6367\n",
      "Epoch [5/100] Batch 0/1875 Loss D: 0.0000, Loss G: 45.0396\n",
      "Epoch [5/100] Batch 625/1875 Loss D: 0.0000, Loss G: 44.9185\n",
      "Epoch [5/100] Batch 1250/1875 Loss D: 0.0000, Loss G: 44.3984\n",
      "Epoch [6/100] Batch 0/1875 Loss D: 0.0000, Loss G: 44.2597\n",
      "Epoch [6/100] Batch 625/1875 Loss D: 0.0000, Loss G: 43.4942\n",
      "Epoch [6/100] Batch 1250/1875 Loss D: 0.0000, Loss G: 38.8586\n",
      "Epoch [7/100] Batch 0/1875 Loss D: 0.0008, Loss G: 7.9301\n",
      "Epoch [7/100] Batch 625/1875 Loss D: 0.0011, Loss G: 9.6488\n",
      "Epoch [7/100] Batch 1250/1875 Loss D: 0.0001, Loss G: 10.9069\n",
      "Epoch [8/100] Batch 0/1875 Loss D: 0.0551, Loss G: 9.6437\n",
      "Epoch [8/100] Batch 625/1875 Loss D: 0.0273, Loss G: 4.7685\n",
      "Epoch [8/100] Batch 1250/1875 Loss D: 0.0325, Loss G: 4.3387\n",
      "Epoch [9/100] Batch 0/1875 Loss D: 0.0368, Loss G: 4.8670\n",
      "Epoch [9/100] Batch 625/1875 Loss D: 0.1056, Loss G: 3.6419\n",
      "Epoch [9/100] Batch 1250/1875 Loss D: 0.0639, Loss G: 4.6577\n",
      "Epoch [10/100] Batch 0/1875 Loss D: 0.1875, Loss G: 6.1216\n",
      "Epoch [10/100] Batch 625/1875 Loss D: 0.0595, Loss G: 2.8090\n",
      "Epoch [10/100] Batch 1250/1875 Loss D: 0.1497, Loss G: 5.3987\n",
      "Epoch [11/100] Batch 0/1875 Loss D: 0.0842, Loss G: 4.6239\n",
      "Epoch [11/100] Batch 625/1875 Loss D: 0.2516, Loss G: 2.0505\n",
      "Epoch [11/100] Batch 1250/1875 Loss D: 0.1141, Loss G: 3.2353\n",
      "Epoch [12/100] Batch 0/1875 Loss D: 0.1108, Loss G: 2.9584\n",
      "Epoch [12/100] Batch 625/1875 Loss D: 0.1107, Loss G: 2.5579\n",
      "Epoch [12/100] Batch 1250/1875 Loss D: 0.0435, Loss G: 3.9132\n",
      "Epoch [13/100] Batch 0/1875 Loss D: 0.0719, Loss G: 4.2418\n",
      "Epoch [13/100] Batch 625/1875 Loss D: 0.0881, Loss G: 2.2965\n",
      "Epoch [13/100] Batch 1250/1875 Loss D: 0.2124, Loss G: 2.3855\n",
      "Epoch [14/100] Batch 0/1875 Loss D: 0.1101, Loss G: 3.3138\n",
      "Epoch [14/100] Batch 625/1875 Loss D: 0.2078, Loss G: 5.3991\n",
      "Epoch [14/100] Batch 1250/1875 Loss D: 0.0689, Loss G: 3.3317\n",
      "Epoch [15/100] Batch 0/1875 Loss D: 0.0681, Loss G: 4.0739\n",
      "Epoch [15/100] Batch 625/1875 Loss D: 0.0623, Loss G: 4.7666\n",
      "Epoch [15/100] Batch 1250/1875 Loss D: 0.0794, Loss G: 4.0281\n",
      "Epoch [16/100] Batch 0/1875 Loss D: 0.0461, Loss G: 3.5866\n",
      "Epoch [16/100] Batch 625/1875 Loss D: 0.1092, Loss G: 4.3244\n",
      "Epoch [16/100] Batch 1250/1875 Loss D: 0.0344, Loss G: 3.9741\n",
      "Epoch [17/100] Batch 0/1875 Loss D: 0.0541, Loss G: 5.1436\n",
      "Epoch [17/100] Batch 625/1875 Loss D: 0.0432, Loss G: 4.9912\n",
      "Epoch [17/100] Batch 1250/1875 Loss D: 0.0586, Loss G: 4.8450\n",
      "Epoch [18/100] Batch 0/1875 Loss D: 0.0380, Loss G: 3.1773\n",
      "Epoch [18/100] Batch 625/1875 Loss D: 0.0533, Loss G: 4.3113\n",
      "Epoch [18/100] Batch 1250/1875 Loss D: 0.0570, Loss G: 4.7706\n",
      "Epoch [19/100] Batch 0/1875 Loss D: 0.0930, Loss G: 3.3745\n",
      "Epoch [19/100] Batch 625/1875 Loss D: 0.0287, Loss G: 5.7445\n",
      "Epoch [19/100] Batch 1250/1875 Loss D: 0.1307, Loss G: 1.8475\n",
      "Epoch [20/100] Batch 0/1875 Loss D: 0.0569, Loss G: 4.3882\n",
      "Epoch [20/100] Batch 625/1875 Loss D: 0.0884, Loss G: 5.0221\n",
      "Epoch [20/100] Batch 1250/1875 Loss D: 0.0230, Loss G: 4.4842\n",
      "Epoch [21/100] Batch 0/1875 Loss D: 0.0962, Loss G: 5.2752\n",
      "Epoch [21/100] Batch 625/1875 Loss D: 0.0335, Loss G: 3.4906\n",
      "Epoch [21/100] Batch 1250/1875 Loss D: 0.2157, Loss G: 3.7221\n",
      "Epoch [22/100] Batch 0/1875 Loss D: 0.2761, Loss G: 4.8361\n",
      "Epoch [22/100] Batch 625/1875 Loss D: 0.0667, Loss G: 4.6584\n",
      "Epoch [22/100] Batch 1250/1875 Loss D: 0.0173, Loss G: 4.4597\n",
      "Epoch [23/100] Batch 0/1875 Loss D: 0.1722, Loss G: 5.1372\n",
      "Epoch [23/100] Batch 625/1875 Loss D: 0.0497, Loss G: 5.1715\n",
      "Epoch [23/100] Batch 1250/1875 Loss D: 0.0298, Loss G: 6.1043\n",
      "Epoch [24/100] Batch 0/1875 Loss D: 0.1806, Loss G: 2.5778\n",
      "Epoch [24/100] Batch 625/1875 Loss D: 0.1330, Loss G: 4.0547\n",
      "Epoch [24/100] Batch 1250/1875 Loss D: 0.0468, Loss G: 3.8190\n",
      "Epoch [25/100] Batch 0/1875 Loss D: 0.0297, Loss G: 4.3077\n",
      "Epoch [25/100] Batch 625/1875 Loss D: 0.0230, Loss G: 3.7570\n",
      "Epoch [25/100] Batch 1250/1875 Loss D: 0.0275, Loss G: 4.8509\n",
      "Epoch [26/100] Batch 0/1875 Loss D: 0.0516, Loss G: 5.4136\n",
      "Epoch [26/100] Batch 625/1875 Loss D: 0.1396, Loss G: 4.8539\n",
      "Epoch [26/100] Batch 1250/1875 Loss D: 0.1737, Loss G: 5.7234\n",
      "Epoch [27/100] Batch 0/1875 Loss D: 0.0279, Loss G: 4.9331\n",
      "Epoch [27/100] Batch 625/1875 Loss D: 0.0611, Loss G: 3.1157\n",
      "Epoch [27/100] Batch 1250/1875 Loss D: 0.0824, Loss G: 4.4776\n",
      "Epoch [28/100] Batch 0/1875 Loss D: 0.1905, Loss G: 5.8528\n",
      "Epoch [28/100] Batch 625/1875 Loss D: 0.0586, Loss G: 3.8669\n",
      "Epoch [28/100] Batch 1250/1875 Loss D: 0.0617, Loss G: 5.8948\n",
      "Epoch [29/100] Batch 0/1875 Loss D: 0.0905, Loss G: 3.2128\n",
      "Epoch [29/100] Batch 625/1875 Loss D: 0.1362, Loss G: 2.7573\n",
      "Epoch [29/100] Batch 1250/1875 Loss D: 0.0531, Loss G: 3.9762\n",
      "Epoch [30/100] Batch 0/1875 Loss D: 0.0080, Loss G: 4.7861\n",
      "Epoch [30/100] Batch 625/1875 Loss D: 0.0755, Loss G: 5.1105\n",
      "Epoch [30/100] Batch 1250/1875 Loss D: 0.0900, Loss G: 3.0621\n",
      "Epoch [31/100] Batch 0/1875 Loss D: 0.0301, Loss G: 4.9967\n",
      "Epoch [31/100] Batch 625/1875 Loss D: 0.0235, Loss G: 4.5321\n",
      "Epoch [31/100] Batch 1250/1875 Loss D: 0.1262, Loss G: 3.6034\n",
      "Epoch [32/100] Batch 0/1875 Loss D: 0.0267, Loss G: 6.1531\n",
      "Epoch [32/100] Batch 625/1875 Loss D: 0.0659, Loss G: 2.8815\n",
      "Epoch [32/100] Batch 1250/1875 Loss D: 0.0191, Loss G: 6.8334\n",
      "Epoch [33/100] Batch 0/1875 Loss D: 0.1716, Loss G: 3.8877\n",
      "Epoch [33/100] Batch 625/1875 Loss D: 0.0314, Loss G: 6.7529\n",
      "Epoch [33/100] Batch 1250/1875 Loss D: 0.0168, Loss G: 5.1084\n",
      "Epoch [34/100] Batch 0/1875 Loss D: 0.0277, Loss G: 5.9979\n",
      "Epoch [34/100] Batch 625/1875 Loss D: 0.0292, Loss G: 5.1769\n",
      "Epoch [34/100] Batch 1250/1875 Loss D: 0.1214, Loss G: 6.5418\n",
      "Epoch [35/100] Batch 0/1875 Loss D: 0.0107, Loss G: 6.8433\n",
      "Epoch [35/100] Batch 625/1875 Loss D: 0.1209, Loss G: 6.5530\n",
      "Epoch [35/100] Batch 1250/1875 Loss D: 0.1529, Loss G: 2.6336\n",
      "Epoch [36/100] Batch 0/1875 Loss D: 0.1626, Loss G: 3.6517\n",
      "Epoch [36/100] Batch 625/1875 Loss D: 0.0715, Loss G: 5.0305\n",
      "Epoch [36/100] Batch 1250/1875 Loss D: 0.0583, Loss G: 4.0006\n",
      "Epoch [37/100] Batch 0/1875 Loss D: 0.1150, Loss G: 4.7326\n",
      "Epoch [37/100] Batch 625/1875 Loss D: 0.0875, Loss G: 7.5588\n",
      "Epoch [37/100] Batch 1250/1875 Loss D: 0.0403, Loss G: 8.9822\n",
      "Epoch [38/100] Batch 0/1875 Loss D: 0.0492, Loss G: 3.5163\n",
      "Epoch [38/100] Batch 625/1875 Loss D: 0.1189, Loss G: 4.2560\n",
      "Epoch [38/100] Batch 1250/1875 Loss D: 0.0328, Loss G: 5.6818\n",
      "Epoch [39/100] Batch 0/1875 Loss D: 0.0710, Loss G: 3.5033\n",
      "Epoch [39/100] Batch 625/1875 Loss D: 0.0334, Loss G: 7.0680\n",
      "Epoch [39/100] Batch 1250/1875 Loss D: 0.1199, Loss G: 4.0907\n",
      "Epoch [40/100] Batch 0/1875 Loss D: 0.0644, Loss G: 6.3139\n",
      "Epoch [40/100] Batch 625/1875 Loss D: 0.0585, Loss G: 3.2208\n",
      "Epoch [40/100] Batch 1250/1875 Loss D: 0.0749, Loss G: 6.9363\n",
      "Epoch [41/100] Batch 0/1875 Loss D: 0.0404, Loss G: 4.5552\n",
      "Epoch [41/100] Batch 625/1875 Loss D: 0.1987, Loss G: 2.4680\n",
      "Epoch [41/100] Batch 1250/1875 Loss D: 0.0952, Loss G: 3.0921\n",
      "Epoch [42/100] Batch 0/1875 Loss D: 0.0077, Loss G: 6.0379\n",
      "Epoch [42/100] Batch 625/1875 Loss D: 0.0349, Loss G: 5.0293\n",
      "Epoch [42/100] Batch 1250/1875 Loss D: 0.0823, Loss G: 5.0965\n",
      "Epoch [43/100] Batch 0/1875 Loss D: 0.0500, Loss G: 4.1387\n",
      "Epoch [43/100] Batch 625/1875 Loss D: 0.0359, Loss G: 4.6963\n",
      "Epoch [43/100] Batch 1250/1875 Loss D: 0.0645, Loss G: 5.0525\n",
      "Epoch [44/100] Batch 0/1875 Loss D: 0.2284, Loss G: 4.5387\n",
      "Epoch [44/100] Batch 625/1875 Loss D: 0.0734, Loss G: 3.7594\n",
      "Epoch [44/100] Batch 1250/1875 Loss D: 0.1328, Loss G: 4.9597\n",
      "Epoch [45/100] Batch 0/1875 Loss D: 0.0279, Loss G: 4.4196\n",
      "Epoch [45/100] Batch 625/1875 Loss D: 0.1339, Loss G: 4.0493\n",
      "Epoch [45/100] Batch 1250/1875 Loss D: 0.0244, Loss G: 4.7986\n",
      "Epoch [46/100] Batch 0/1875 Loss D: 0.0733, Loss G: 2.9791\n",
      "Epoch [46/100] Batch 625/1875 Loss D: 0.0119, Loss G: 4.6137\n",
      "Epoch [46/100] Batch 1250/1875 Loss D: 0.2617, Loss G: 7.4634\n",
      "Epoch [47/100] Batch 0/1875 Loss D: 0.0644, Loss G: 4.9939\n",
      "Epoch [47/100] Batch 625/1875 Loss D: 0.2122, Loss G: 8.9238\n",
      "Epoch [47/100] Batch 1250/1875 Loss D: 0.0937, Loss G: 3.5988\n",
      "Epoch [48/100] Batch 0/1875 Loss D: 0.0418, Loss G: 5.3284\n",
      "Epoch [48/100] Batch 625/1875 Loss D: 0.0133, Loss G: 7.2723\n",
      "Epoch [48/100] Batch 1250/1875 Loss D: 0.7903, Loss G: 4.6986\n",
      "Epoch [49/100] Batch 0/1875 Loss D: 0.0424, Loss G: 5.5277\n",
      "Epoch [49/100] Batch 625/1875 Loss D: 0.0520, Loss G: 3.5500\n",
      "Epoch [49/100] Batch 1250/1875 Loss D: 0.0420, Loss G: 3.6727\n",
      "Epoch [50/100] Batch 0/1875 Loss D: 0.0572, Loss G: 4.4777\n",
      "Epoch [50/100] Batch 625/1875 Loss D: 0.1023, Loss G: 8.5816\n",
      "Epoch [50/100] Batch 1250/1875 Loss D: 0.0340, Loss G: 5.8948\n",
      "Epoch [51/100] Batch 0/1875 Loss D: 0.1114, Loss G: 5.5834\n",
      "Epoch [51/100] Batch 625/1875 Loss D: 0.0127, Loss G: 6.5867\n",
      "Epoch [51/100] Batch 1250/1875 Loss D: 0.0220, Loss G: 8.1372\n",
      "Epoch [52/100] Batch 0/1875 Loss D: 0.0541, Loss G: 4.0862\n",
      "Epoch [52/100] Batch 625/1875 Loss D: 0.0766, Loss G: 4.7479\n",
      "Epoch [52/100] Batch 1250/1875 Loss D: 0.1788, Loss G: 6.8300\n",
      "Epoch [53/100] Batch 0/1875 Loss D: 0.1282, Loss G: 4.1145\n",
      "Epoch [53/100] Batch 625/1875 Loss D: 0.0450, Loss G: 5.9333\n",
      "Epoch [53/100] Batch 1250/1875 Loss D: 0.0152, Loss G: 5.9242\n",
      "Epoch [54/100] Batch 0/1875 Loss D: 0.1213, Loss G: 4.8284\n",
      "Epoch [54/100] Batch 625/1875 Loss D: 0.0138, Loss G: 4.2585\n",
      "Epoch [54/100] Batch 1250/1875 Loss D: 0.1338, Loss G: 4.9123\n",
      "Epoch [55/100] Batch 0/1875 Loss D: 0.0324, Loss G: 3.9790\n",
      "Epoch [55/100] Batch 625/1875 Loss D: 0.0763, Loss G: 4.1992\n",
      "Epoch [55/100] Batch 1250/1875 Loss D: 0.0627, Loss G: 4.2442\n",
      "Epoch [56/100] Batch 0/1875 Loss D: 0.5127, Loss G: 8.3351\n",
      "Epoch [56/100] Batch 625/1875 Loss D: 0.0237, Loss G: 5.2245\n",
      "Epoch [56/100] Batch 1250/1875 Loss D: 0.0207, Loss G: 5.6107\n",
      "Epoch [57/100] Batch 0/1875 Loss D: 0.0546, Loss G: 6.0864\n",
      "Epoch [57/100] Batch 625/1875 Loss D: 0.1154, Loss G: 3.9939\n",
      "Epoch [57/100] Batch 1250/1875 Loss D: 0.2485, Loss G: 3.5670\n",
      "Epoch [58/100] Batch 0/1875 Loss D: 0.1019, Loss G: 8.5683\n",
      "Epoch [58/100] Batch 625/1875 Loss D: 0.0265, Loss G: 4.2445\n",
      "Epoch [58/100] Batch 1250/1875 Loss D: 0.0499, Loss G: 8.5396\n",
      "Epoch [59/100] Batch 0/1875 Loss D: 0.0738, Loss G: 4.8270\n",
      "Epoch [59/100] Batch 625/1875 Loss D: 0.0766, Loss G: 7.9072\n",
      "Epoch [59/100] Batch 1250/1875 Loss D: 0.1683, Loss G: 3.5265\n",
      "Epoch [60/100] Batch 0/1875 Loss D: 0.0584, Loss G: 7.0236\n",
      "Epoch [60/100] Batch 625/1875 Loss D: 0.2638, Loss G: 2.9015\n",
      "Epoch [60/100] Batch 1250/1875 Loss D: 0.0314, Loss G: 5.0828\n",
      "Epoch [61/100] Batch 0/1875 Loss D: 0.2573, Loss G: 2.3219\n",
      "Epoch [61/100] Batch 625/1875 Loss D: 0.2071, Loss G: 2.7826\n",
      "Epoch [61/100] Batch 1250/1875 Loss D: 0.0302, Loss G: 6.4455\n",
      "Epoch [62/100] Batch 0/1875 Loss D: 0.2945, Loss G: 4.1717\n",
      "Epoch [62/100] Batch 625/1875 Loss D: 0.0327, Loss G: 5.8286\n",
      "Epoch [62/100] Batch 1250/1875 Loss D: 0.0507, Loss G: 3.6850\n",
      "Epoch [63/100] Batch 0/1875 Loss D: 0.0664, Loss G: 3.2292\n",
      "Epoch [63/100] Batch 625/1875 Loss D: 0.0659, Loss G: 4.9766\n",
      "Epoch [63/100] Batch 1250/1875 Loss D: 0.0513, Loss G: 3.7991\n",
      "Epoch [64/100] Batch 0/1875 Loss D: 0.1681, Loss G: 4.3405\n",
      "Epoch [64/100] Batch 625/1875 Loss D: 0.0662, Loss G: 5.5843\n",
      "Epoch [64/100] Batch 1250/1875 Loss D: 0.0870, Loss G: 3.2460\n",
      "Epoch [65/100] Batch 0/1875 Loss D: 0.1131, Loss G: 3.9605\n",
      "Epoch [65/100] Batch 625/1875 Loss D: 0.0825, Loss G: 3.7065\n",
      "Epoch [65/100] Batch 1250/1875 Loss D: 0.0095, Loss G: 4.4966\n",
      "Epoch [66/100] Batch 0/1875 Loss D: 0.1278, Loss G: 8.1850\n",
      "Epoch [66/100] Batch 625/1875 Loss D: 0.0996, Loss G: 4.9717\n",
      "Epoch [66/100] Batch 1250/1875 Loss D: 0.0906, Loss G: 5.7610\n",
      "Epoch [67/100] Batch 0/1875 Loss D: 0.1137, Loss G: 3.3944\n",
      "Epoch [67/100] Batch 625/1875 Loss D: 0.0409, Loss G: 3.9402\n",
      "Epoch [67/100] Batch 1250/1875 Loss D: 0.0570, Loss G: 4.2527\n",
      "Epoch [68/100] Batch 0/1875 Loss D: 0.0460, Loss G: 4.4469\n",
      "Epoch [68/100] Batch 625/1875 Loss D: 0.0448, Loss G: 4.2829\n",
      "Epoch [68/100] Batch 1250/1875 Loss D: 0.0245, Loss G: 4.1792\n",
      "Epoch [69/100] Batch 0/1875 Loss D: 0.0483, Loss G: 4.5663\n",
      "Epoch [69/100] Batch 625/1875 Loss D: 0.1076, Loss G: 3.4160\n",
      "Epoch [69/100] Batch 1250/1875 Loss D: 0.0489, Loss G: 4.4720\n",
      "Epoch [70/100] Batch 0/1875 Loss D: 0.1172, Loss G: 5.6758\n",
      "Epoch [70/100] Batch 625/1875 Loss D: 0.0181, Loss G: 6.5378\n",
      "Epoch [70/100] Batch 1250/1875 Loss D: 0.0757, Loss G: 6.0869\n",
      "Epoch [71/100] Batch 0/1875 Loss D: 0.0963, Loss G: 2.7779\n",
      "Epoch [71/100] Batch 625/1875 Loss D: 0.0731, Loss G: 4.0485\n",
      "Epoch [71/100] Batch 1250/1875 Loss D: 0.0473, Loss G: 4.5513\n",
      "Epoch [72/100] Batch 0/1875 Loss D: 0.0380, Loss G: 6.7019\n",
      "Epoch [72/100] Batch 625/1875 Loss D: 0.0690, Loss G: 3.5863\n",
      "Epoch [72/100] Batch 1250/1875 Loss D: 0.0306, Loss G: 4.5874\n",
      "Epoch [73/100] Batch 0/1875 Loss D: 0.1735, Loss G: 3.7260\n",
      "Epoch [73/100] Batch 625/1875 Loss D: 0.0951, Loss G: 4.1721\n",
      "Epoch [73/100] Batch 1250/1875 Loss D: 0.0604, Loss G: 3.2855\n",
      "Epoch [74/100] Batch 0/1875 Loss D: 0.0390, Loss G: 5.5001\n",
      "Epoch [74/100] Batch 625/1875 Loss D: 0.0530, Loss G: 4.5949\n",
      "Epoch [74/100] Batch 1250/1875 Loss D: 0.2838, Loss G: 2.2348\n",
      "Epoch [75/100] Batch 0/1875 Loss D: 0.0845, Loss G: 4.5507\n",
      "Epoch [75/100] Batch 625/1875 Loss D: 0.1130, Loss G: 6.1517\n",
      "Epoch [75/100] Batch 1250/1875 Loss D: 0.0239, Loss G: 4.5411\n",
      "Epoch [76/100] Batch 0/1875 Loss D: 0.0361, Loss G: 5.8866\n",
      "Epoch [76/100] Batch 625/1875 Loss D: 0.0515, Loss G: 4.0628\n",
      "Epoch [76/100] Batch 1250/1875 Loss D: 0.0577, Loss G: 7.6893\n",
      "Epoch [77/100] Batch 0/1875 Loss D: 0.0509, Loss G: 4.1240\n",
      "Epoch [77/100] Batch 625/1875 Loss D: 0.0891, Loss G: 5.3952\n",
      "Epoch [77/100] Batch 1250/1875 Loss D: 0.0898, Loss G: 4.2049\n",
      "Epoch [78/100] Batch 0/1875 Loss D: 0.0939, Loss G: 4.9824\n",
      "Epoch [78/100] Batch 625/1875 Loss D: 0.0281, Loss G: 4.4323\n",
      "Epoch [78/100] Batch 1250/1875 Loss D: 0.0107, Loss G: 5.5090\n",
      "Epoch [79/100] Batch 0/1875 Loss D: 0.0418, Loss G: 4.7078\n",
      "Epoch [79/100] Batch 625/1875 Loss D: 0.0659, Loss G: 5.1219\n",
      "Epoch [79/100] Batch 1250/1875 Loss D: 0.0969, Loss G: 4.6924\n",
      "Epoch [80/100] Batch 0/1875 Loss D: 0.0361, Loss G: 5.0645\n",
      "Epoch [80/100] Batch 625/1875 Loss D: 0.0421, Loss G: 5.5664\n",
      "Epoch [80/100] Batch 1250/1875 Loss D: 0.0115, Loss G: 6.5282\n",
      "Epoch [81/100] Batch 0/1875 Loss D: 0.0452, Loss G: 5.9226\n",
      "Epoch [81/100] Batch 625/1875 Loss D: 0.3136, Loss G: 6.8582\n",
      "Epoch [81/100] Batch 1250/1875 Loss D: 0.2912, Loss G: 6.4008\n",
      "Epoch [82/100] Batch 0/1875 Loss D: 0.0611, Loss G: 2.9211\n",
      "Epoch [82/100] Batch 625/1875 Loss D: 0.1157, Loss G: 5.5535\n",
      "Epoch [82/100] Batch 1250/1875 Loss D: 0.1954, Loss G: 9.9577\n",
      "Epoch [83/100] Batch 0/1875 Loss D: 0.0600, Loss G: 5.3346\n",
      "Epoch [83/100] Batch 625/1875 Loss D: 0.0106, Loss G: 6.2354\n",
      "Epoch [83/100] Batch 1250/1875 Loss D: 0.0215, Loss G: 5.0673\n",
      "Epoch [84/100] Batch 0/1875 Loss D: 0.0321, Loss G: 4.4865\n",
      "Epoch [84/100] Batch 625/1875 Loss D: 0.0392, Loss G: 6.9415\n",
      "Epoch [84/100] Batch 1250/1875 Loss D: 0.0649, Loss G: 6.3396\n",
      "Epoch [85/100] Batch 0/1875 Loss D: 0.0407, Loss G: 5.9434\n",
      "Epoch [85/100] Batch 625/1875 Loss D: 0.0481, Loss G: 5.2770\n",
      "Epoch [85/100] Batch 1250/1875 Loss D: 0.0295, Loss G: 4.8279\n",
      "Epoch [86/100] Batch 0/1875 Loss D: 0.0535, Loss G: 8.7395\n",
      "Epoch [86/100] Batch 625/1875 Loss D: 0.0513, Loss G: 5.1900\n",
      "Epoch [86/100] Batch 1250/1875 Loss D: 0.1426, Loss G: 5.6467\n",
      "Epoch [87/100] Batch 0/1875 Loss D: 0.0131, Loss G: 7.1958\n",
      "Epoch [87/100] Batch 625/1875 Loss D: 0.0492, Loss G: 5.7328\n",
      "Epoch [87/100] Batch 1250/1875 Loss D: 0.0515, Loss G: 5.6206\n",
      "Epoch [88/100] Batch 0/1875 Loss D: 0.0368, Loss G: 5.6494\n",
      "Epoch [88/100] Batch 625/1875 Loss D: 0.1293, Loss G: 3.7691\n",
      "Epoch [88/100] Batch 1250/1875 Loss D: 0.0562, Loss G: 3.2830\n",
      "Epoch [89/100] Batch 0/1875 Loss D: 0.0877, Loss G: 4.3315\n",
      "Epoch [89/100] Batch 625/1875 Loss D: 0.0948, Loss G: 4.5452\n",
      "Epoch [89/100] Batch 1250/1875 Loss D: 0.1358, Loss G: 4.7872\n",
      "Epoch [90/100] Batch 0/1875 Loss D: 0.0257, Loss G: 4.7902\n",
      "Epoch [90/100] Batch 625/1875 Loss D: 0.0327, Loss G: 6.6693\n",
      "Epoch [90/100] Batch 1250/1875 Loss D: 0.1785, Loss G: 8.9844\n",
      "Epoch [91/100] Batch 0/1875 Loss D: 0.0299, Loss G: 6.2594\n",
      "Epoch [91/100] Batch 625/1875 Loss D: 0.0366, Loss G: 3.7443\n",
      "Epoch [91/100] Batch 1250/1875 Loss D: 0.2690, Loss G: 10.0967\n",
      "Epoch [92/100] Batch 0/1875 Loss D: 0.0399, Loss G: 4.7688\n",
      "Epoch [92/100] Batch 625/1875 Loss D: 0.0260, Loss G: 6.8052\n",
      "Epoch [92/100] Batch 1250/1875 Loss D: 0.1403, Loss G: 3.5100\n",
      "Epoch [93/100] Batch 0/1875 Loss D: 0.1021, Loss G: 3.4349\n",
      "Epoch [93/100] Batch 625/1875 Loss D: 0.0480, Loss G: 6.6810\n",
      "Epoch [93/100] Batch 1250/1875 Loss D: 0.1797, Loss G: 4.3351\n",
      "Epoch [94/100] Batch 0/1875 Loss D: 0.2692, Loss G: 5.5967\n",
      "Epoch [94/100] Batch 625/1875 Loss D: 0.0829, Loss G: 8.9678\n",
      "Epoch [94/100] Batch 1250/1875 Loss D: 0.0492, Loss G: 5.1341\n",
      "Epoch [95/100] Batch 0/1875 Loss D: 0.0189, Loss G: 5.1083\n",
      "Epoch [95/100] Batch 625/1875 Loss D: 0.0594, Loss G: 3.2269\n",
      "Epoch [95/100] Batch 1250/1875 Loss D: 0.0190, Loss G: 9.0845\n",
      "Epoch [96/100] Batch 0/1875 Loss D: 0.0830, Loss G: 5.2603\n",
      "Epoch [96/100] Batch 625/1875 Loss D: 0.0555, Loss G: 4.8446\n",
      "Epoch [96/100] Batch 1250/1875 Loss D: 0.0206, Loss G: 6.6785\n",
      "Epoch [97/100] Batch 0/1875 Loss D: 0.0839, Loss G: 3.5730\n",
      "Epoch [97/100] Batch 625/1875 Loss D: 0.1896, Loss G: 3.3938\n",
      "Epoch [97/100] Batch 1250/1875 Loss D: 0.1458, Loss G: 5.2746\n",
      "Epoch [98/100] Batch 0/1875 Loss D: 0.0133, Loss G: 5.5276\n",
      "Epoch [98/100] Batch 625/1875 Loss D: 0.0242, Loss G: 4.5845\n",
      "Epoch [98/100] Batch 1250/1875 Loss D: 0.0518, Loss G: 5.1152\n",
      "Epoch [99/100] Batch 0/1875 Loss D: 0.1610, Loss G: 3.1350\n",
      "Epoch [99/100] Batch 625/1875 Loss D: 0.0503, Loss G: 7.2082\n",
      "Epoch [99/100] Batch 1250/1875 Loss D: 0.3656, Loss G: 9.4678\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Configuration settings\n",
    "class Config:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    lr = 3e-4\n",
    "    zDim = 128  # Latent vector size\n",
    "    imageDim = 28\n",
    "    batchSize = 32\n",
    "    numEpochs = 100\n",
    "    logstep = 625\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Define transformations\n",
    "myTransform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "dataset = datasets.MNIST(root=\"dataset/\", transform=myTransform, download=True)\n",
    "loader = DataLoader(dataset, batch_size=config.batchSize, shuffle=True)\n",
    "\n",
    "# CNN-based Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# CNN-based Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, zDim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(zDim, 128 * 7 * 7),\n",
    "            nn.BatchNorm1d(128 * 7 * 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (128, 7, 7)),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize models\n",
    "discriminator = Discriminator().to(config.device)\n",
    "generator = Generator(config.zDim).to(config.device)\n",
    "fixedNoise = torch.randn((config.batchSize, config.zDim)).to(config.device)\n",
    "\n",
    "# Optimizers and loss function\n",
    "optDisc = optim.Adam(discriminator.parameters(), lr=config.lr)\n",
    "optGen = optim.Adam(generator.parameters(), lr=config.lr)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Summary writers\n",
    "writerFake = SummaryWriter(\"C:\\\\Users\\\\Muraa\\\\Desktop\\\\CNN\\\\Fake\")\n",
    "writerReal = SummaryWriter(\"C:\\\\Users\\\\Muraa\\\\Desktop\\\\CNN\\\\Real\")\n",
    "\n",
    "print(\"Started Training and Visualization...\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(config.numEpochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.to(config.device)\n",
    "        batch_size = real.size(0)\n",
    "\n",
    "        # Train Discriminator\n",
    "        noise = torch.randn(batch_size, config.zDim).to(config.device)\n",
    "        fake = generator(noise)\n",
    "\n",
    "        discReal = discriminator(real).view(-1)\n",
    "        discFake = discriminator(fake.detach()).view(-1)\n",
    "\n",
    "        lossDreal = criterion(discReal, torch.ones_like(discReal))\n",
    "        lossDfake = criterion(discFake, torch.zeros_like(discFake))\n",
    "        lossD = (lossDreal + lossDfake) / 2\n",
    "\n",
    "        discriminator.zero_grad()\n",
    "        lossD.backward()\n",
    "        optDisc.step()\n",
    "\n",
    "        # Train Generator\n",
    "        output = discriminator(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "\n",
    "        generator.zero_grad()\n",
    "        lossG.backward()\n",
    "        optGen.step()\n",
    "\n",
    "        # Logging and TensorBoard visualization\n",
    "        if batch_idx % config.logstep == 0:\n",
    "            print(f\"Epoch [{epoch}/{config.numEpochs}] Batch {batch_idx}/{len(loader)} Loss D: {lossD:.4f}, Loss G: {lossG:.4f}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = generator(fixedNoise).reshape(-1, 1, 28, 28)\n",
    "                real = real.reshape(-1, 1, 28, 28)\n",
    "                \n",
    "                imgGridFake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                imgGridReal = torchvision.utils.make_grid(real, normalize=True)\n",
    "                \n",
    "                writerFake.add_image(\"MNIST Fake Images\", imgGridFake, global_step=step)\n",
    "                writerReal.add_image(\"MNIST Real Images\", imgGridReal, global_step=step)\n",
    "\n",
    "                writerFake.flush()\n",
    "                writerReal.flush()\n",
    "\n",
    "            step += 1\n",
    "\n",
    "writerFake.close()\n",
    "writerReal.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c26c7d1-c352-42d8-945f-b07799b8bb58",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (3154903059.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    tensorboard --logdir=\"C:\\Users\\Muraa\\Desktop\\CNN\" --bind_all\u001b[0m\n\u001b[1;37m                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir=\"C:\\Users\\Muraa\\Desktop\\CNN\" --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9424b-ae37-4074-bd7a-b2e537733b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "set TF_ENABLE_ONEDNN_OPTS=0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
